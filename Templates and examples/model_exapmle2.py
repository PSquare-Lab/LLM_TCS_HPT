#!/usr/bin/env python3

import os
import json
import sys
import random
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader, Dataset
from torchvision.models import resnet18
from torchvision import transforms
from torchvision.transforms import functional as F
from PIL import Image
import glob
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

TEMP_DIR = os.environ.get("TEMP_DIR", "temp")

# -----------------------------
# Paper-accurate Scale Jittering
# -----------------------------
class RandomScaleJitter:
    """
    Resize so that the *shorter side* is a random integer in [min_size, max_size],
    preserving the aspect ratio (as done in the original ResNet paper for ImageNet).
    """
    def __init__(self, min_size=256, max_size=480, interpolation=Image.BILINEAR):
        self.min_size = int(min_size)
        self.max_size = int(max_size)
        self.interpolation = interpolation

    def __call__(self, img: Image.Image) -> Image.Image:
        target_short = random.randint(self.min_size, self.max_size)
        w, h = img.size
        if w <= 0 or h <= 0:
            return img
        if w < h:
            new_w = target_short
            new_h = int(round(h * (target_short / w)))
        else:
            new_h = target_short
            new_w = int(round(w * (target_short / h)))
        return F.resize(img, (new_h, new_w), self.interpolation)

def get_hyperparameters():
    """Load hyperparameters from the optimization pipeline with paper-compliant defaults."""
    with open(os.path.join(TEMP_DIR, 'hyperparameters.json'), 'r') as f:
        hyperparams = json.load(f)
    
    # Paper-specific defaults (overriding any non-compliant values)
    paper_defaults = {
        'learning_rate': 0.1,        # Paper uses 0.1 initial LR
        'momentum': 0.9,             # Paper specifies 0.9 momentum
        'weight_decay': 0.0001,      # Paper uses 1e-4
        'optimizer': 'SGD',          # Paper uses SGD only
        'global_pool': 'avg',        # 'avg' or 'max'
        # Expose jitter range as optional knobs (defaults match paper)
        'scale_jitter_min': 256,
        'scale_jitter_max': 480
    }
    
    for key, value in paper_defaults.items():
        if key not in hyperparams:
            hyperparams[key] = value
    
    # Override with optimized values from environment variables if available
    optimized_lr = os.environ.get('OPTIMIZED_LEARNING_RATE')
    optimized_bs = os.environ.get('OPTIMIZED_BATCH_SIZE')
    optimized_wd = os.environ.get('OPTIMIZED_WEIGHT_DECAY')
    optimized_dr = os.environ.get('OPTIMIZED_DROPOUT_RATE')
    optimized_gp = os.environ.get('OPTIMIZED_GLOBAL_POOL')
    
    if optimized_lr and optimized_lr != "":
        hyperparams['learning_rate'] = float(optimized_lr)
        print(f"ðŸŽ¯ Using optimized learning_rate: {optimized_lr}")
    if optimized_bs and optimized_bs != "":
        hyperparams['batch_size'] = int(optimized_bs)
        print(f"ðŸŽ¯ Using optimized batch_size: {optimized_bs}")
    if optimized_wd and optimized_wd != "":
        hyperparams['weight_decay'] = float(optimized_wd)
        print(f"ðŸŽ¯ Using optimized weight_decay: {optimized_wd}")
    if optimized_dr and optimized_dr != "":
        hyperparams['dropout_rate'] = float(optimized_dr)
        print(f"ðŸŽ¯ Using optimized dropout_rate: {optimized_dr}")
    if optimized_gp and optimized_gp != "":
        hyperparams['global_pool'] = optimized_gp
        print(f"ðŸŽ¯ Using optimized global_pool: {optimized_gp}")
    
    # Override with environment variables if set (for current iteration)
    for key in hyperparams:
        env_value = os.environ.get(key)
        if env_value is not None:
            if key in ['batch_size', 'epochs', 'scale_jitter_min', 'scale_jitter_max']:
                hyperparams[key] = int(env_value)
            elif key in ['learning_rate', 'weight_decay', 'momentum', 'dropout_rate']:
                hyperparams[key] = float(env_value)
            else:
                hyperparams[key] = env_value
    
    return hyperparams

def save_results(metrics, epochs):
    """Save training results for the optimization pipeline."""
    results = {
        'metrics': metrics,
        'epochs': epochs
    }
    os.makedirs(TEMP_DIR, exist_ok=True)
    filepath = os.path.join(TEMP_DIR, 'results.json')
    try:
        with open(filepath, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"âœ… Results saved to {filepath}")
    except Exception as e:
        print(f"âŒ Error saving results: {e}")

class ButterflyDataset(Dataset):
    """Custom dataset for butterfly images with better error handling."""
    def __init__(self, image_paths, labels, transform=None, root_dir=None):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform
        self.root_dir = root_dir
        
        self.valid_indices = []
        for i, label in enumerate(labels):
            if label != -1:
                self.valid_indices.append(i)
    
    def __len__(self):
        return len(self.valid_indices)
    
    def __getitem__(self, idx):
        actual_idx = self.valid_indices[idx]
        image_path = self.image_paths[actual_idx]
        label = self.labels[actual_idx]
        image_path = os.path.normpath(image_path)
        
        try:
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            print(f"âš ï¸ Error loading {os.path.basename(image_path)}: {e}")
            image = Image.new('RGB', (224, 224), (0, 0, 0))
        
        if self.transform:
            image = self.transform(image)
        
        return image, label

def load_data_from_csv(data_dir):
    """Load dataset from CSVs and encode labels to integers. FIXED VERSION - filters missing files."""
    try:
        train_csv_path = os.path.join(data_dir, 'Training_set.csv')
        test_csv_path = os.path.join(data_dir, 'Testing_set.csv')

        train_df = pd.read_csv(train_csv_path)
        test_df = pd.read_csv(test_csv_path)
        
        print(f"ðŸ“Š CSV loaded - Training: {len(train_df)}, Testing: {len(test_df)}")

        has_test_labels = 'label' in test_df.columns
        print(f"ðŸ“Š Test set has labels: {has_test_labels}")

        def build_train_path(filename):
            if 'train' in filename or data_dir in filename:
                return filename
            return os.path.join(data_dir, 'train', filename)
        
        def build_test_path(filename):
            if 'test' in filename or 'train' in filename or data_dir in filename:
                return filename
            test_dir = os.path.join(data_dir, 'test')
            if os.path.exists(test_dir):
                return os.path.join(data_dir, 'test', filename)
            else:
                return os.path.join(data_dir, 'train', filename)
        
        train_df['full_path'] = train_df['filename'].apply(build_train_path)
        test_df['full_path'] = test_df['filename'].apply(build_test_path)

        print("ðŸ” Filtering missing files...")
        
        train_exists_mask = train_df['full_path'].apply(os.path.exists)
        train_df_filtered = train_df[train_exists_mask].copy()
        
        test_exists_mask = test_df['full_path'].apply(os.path.exists)
        test_df_filtered = test_df[test_exists_mask].copy()
        
        print(f"ðŸ“Š Files filtered - Training: {len(train_df)} -> {len(train_df_filtered)} ({len(train_df) - len(train_df_filtered)} missing)")
        print(f"ðŸ“Š Files filtered - Testing: {len(test_df)} -> {len(test_df_filtered)} ({len(test_df) - len(test_df_filtered)} missing)")
        
        if len(train_df_filtered) == 0:
            raise ValueError("No training images found after filtering missing files!")

        le = LabelEncoder()
        train_df_filtered['label_encoded'] = le.fit_transform(train_df_filtered['label'])
        
        if has_test_labels and len(test_df_filtered) > 0:
            test_labels_valid = test_df_filtered['label'].isin(le.classes_)
            if test_labels_valid.sum() > 0:
                test_df_filtered = test_df_filtered[test_labels_valid].copy()
                test_df_filtered['label_encoded'] = le.transform(test_df_filtered['label'])
            else:
                test_df_filtered['label_encoded'] = -1
                print("âš ï¸ No valid test labels found in training classes")
        else:
            test_df_filtered['label_encoded'] = -1
            print("âš ï¸ Test set has no labels - using dummy labels")

        sample_train_paths = train_df_filtered['full_path'].head(3).tolist()
        print("ðŸ” Sample verified paths:")
        for i, path in enumerate(sample_train_paths):
            exists = os.path.exists(path)
            print(f"   Train {i+1}: {os.path.basename(path)} {'âœ…' if exists else 'âŒ'}")

        X_train, X_val, y_train, y_val = train_test_split(
            train_df_filtered['full_path'].values,
            train_df_filtered['label_encoded'].values,
            test_size=0.1,
            random_state=42,
            stratify=train_df_filtered['label_encoded'].values
        )

        X_test = test_df_filtered['full_path'].values
        y_test = test_df_filtered['label_encoded'].values

        num_classes = len(le.classes_)
        class_names = list(le.classes_)

        print(f"ðŸ“Š Final dataset - Classes: {num_classes}, Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}")
        print(f"ðŸ“Š Class distribution: {len(train_df_filtered)} samples across {num_classes} classes")
        
        return X_train, X_val, X_test, y_train, y_val, y_test, num_classes, class_names, le

    except Exception as e:
        print(f"âŒ Error loading data: {str(e)}")
        raise e

def get_class_names_and_paths(data_dir):
    """Get class names and image paths from directory structure."""
    class_dirs = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]
    class_dirs.sort()
    
    class_to_idx = {cls_name: idx for idx, cls_name in enumerate(class_dirs)}
    
    image_paths = []
    labels = []
    
    for class_name in class_dirs:
        class_dir = os.path.join(data_dir, class_name)
        for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff']:
            pattern = os.path.join(class_dir, ext)
            class_images = glob.glob(pattern)
            image_paths.extend(class_images)
            labels.extend([class_to_idx[class_name]] * len(class_images))
    
    return image_paths, labels, class_to_idx, class_dirs

def handle_test_dataset_with_no_labels(X_test, y_test, transform, root_dir):
    """Handle test dataset that may not have real labels."""
    if len(y_test) > 0 and all(label == -1 for label in y_test):
        print("âš ï¸ Creating inference dataset for unlabeled test set")
        
        class InferenceDataset(Dataset):
            def __init__(self, image_paths, transform=None, root_dir=None):
                self.image_paths = image_paths
                self.transform = transform
                self.root_dir = root_dir
            
            def __len__(self):
                return len(self.image_paths)
            
            def __getitem__(self, idx):
                image_path = self.image_paths[idx]
                image_path = os.path.normpath(image_path)
                try:
                    image = Image.open(image_path).convert('RGB')
                except Exception as e:
                    print(f"âš ï¸ Error loading {os.path.basename(image_path)}: {e}")
                    image = Image.new('RGB', (224, 224), (0, 0, 0))
                if self.transform:
                    image = self.transform(image)
                return image, 0
        
        return InferenceDataset(X_test, transform=transform, root_dir=root_dir)
    else:
        return ButterflyDataset(X_test, y_test, transform=transform, root_dir=root_dir)

def load_butterfly_data(data_dir, batch_size, test_size=0.2, val_size=0.1, scale_jitter_min=256, scale_jitter_max=480):
    """
    Load and prepare butterfly dataset with PAPER-COMPLIANT data augmentation.

    Following the original ResNet paper:
    - Training: Scale jittering (shorter side âˆˆ [256, 480]) â†’ Random 224x224 crop â†’ random horizontal flip â†’ normalize
    - Testing: Resize shorter side to 256 â†’ 224x224 center crop â†’ normalize
    - NO non-paper augmentations like ColorJitter or rotation
    """
    try:
        print("ðŸ¦‹ Loading butterfly dataset with paper-compliant augmentation (incl. Scale Jittering)...")
        
        train_csv_path = os.path.join(data_dir, 'Training_set.csv')
        test_csv_path = os.path.join(data_dir, 'Testing_set.csv')
        
        if os.path.exists(train_csv_path) and os.path.exists(test_csv_path):
            X_train, X_val, X_test, y_train, y_val, y_test, num_classes, class_names, le = load_data_from_csv(data_dir)
            if X_train is None:
                raise ValueError("Failed to load CSV-based dataset")
        else:
            print("ðŸ“ CSV files not found, using directory structure...")
            image_paths, labels, class_to_idx, class_names = get_class_names_and_paths(data_dir)
            if len(image_paths) == 0:
                raise ValueError(f"No images found in {data_dir}")
            print(f"ðŸ“Š Found {len(image_paths)} images across {len(class_names)} classes")
            
            X_temp, X_test, y_temp, y_test = train_test_split(
                image_paths, labels, test_size=test_size, random_state=42, stratify=labels
            )
            val_ratio = val_size / (1 - test_size)
            X_train, X_val, y_train, y_val = train_test_split(
                X_temp, y_temp, test_size=val_ratio, random_state=42, stratify=y_temp
            )
            num_classes = len(class_names)
            le = None
        
        print(f"âœ… Dataset loaded: {len(X_train)} train, {len(X_val)} val, {len(X_test)} test")
        
        # PAPER-COMPLIANT Data transformations
        # TRAIN: scale jittering (shorter side âˆˆ [256, 480]) â†’ RandomCrop(224) â†’ RandomHorizontalFlip
        transform_train = transforms.Compose([
            RandomScaleJitter(scale_jitter_min, scale_jitter_max),
            transforms.RandomCrop(224),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # TEST: shorter side to 256 â†’ CenterCrop(224)
        transform_test = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        print("ðŸ“Š Using paper-compliant augmentation:")
        print(f"   - Training: ScaleJitter(shorterâˆˆ[{scale_jitter_min},{scale_jitter_max}]) â†’ RandomCrop(224) â†’ RandomHorizontalFlip")
        print("   - Testing: Resize(256-shorter) â†’ CenterCrop(224)")
        print("   - Removed: ColorJitter, Rotation (not in original paper)")
        
        train_dataset = ButterflyDataset(X_train, y_train, transform=transform_train, root_dir=data_dir)
        val_dataset = ButterflyDataset(X_val, y_val, transform=transform_test, root_dir=data_dir)
        test_dataset = handle_test_dataset_with_no_labels(X_test, y_test, transform=transform_test, root_dir=data_dir)
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
        
        return train_loader, val_loader, test_loader, num_classes, class_names
        
    except Exception as e:
        print(f"âŒ Failed to load dataset: {e}")
        sys.exit(1)

def evaluate_model_safe(model, data_loader, criterion, device, has_real_labels=True):
    """Evaluate model with handling for datasets without real labels."""
    model.eval()
    total_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for inputs, labels in data_loader:
            inputs = inputs.to(device)
            outputs = model(inputs)
            
            if has_real_labels and not all(label == -1 for label in labels):
                labels = labels.to(device)
                loss = criterion(outputs, labels)
                total_loss += loss.item()
                
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
            else:
                total += inputs.size(0)
    
    if has_real_labels and total > 0:
        avg_loss = total_loss / len(data_loader)
        accuracy = correct / total
        return avg_loss, accuracy
    else:
        return 0.0, 0.0

def evaluate_model(model, data_loader, criterion, device):
    """Evaluate model on given data loader."""
    model.eval()
    total_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for inputs, labels in data_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            total_loss += loss.item()
            
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    avg_loss = total_loss / len(data_loader)
    accuracy = correct / total
    return avg_loss, accuracy

class ResNetPaperCompliant(nn.Module):
    """
    ResNet-18 following the EXACT specifications from the original paper.

    Key changes:
    - Configurable global pooling (avg or max)
    - Optional dropout (not in paper, default 0)
    - Kaiming initialization
    """
    def __init__(self, num_classes=10, dropout_rate=0.0, global_pool='avg', pretrained=True):
        super(ResNetPaperCompliant, self).__init__()
        # Note: If using torchvision>=0.13, consider resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)
        self.resnet = resnet18(pretrained=pretrained)
        self.dropout_rate = dropout_rate
        self.global_pool = global_pool
        
        num_features = self.resnet.fc.in_features
        self.resnet = nn.Sequential(*list(self.resnet.children())[:-2])  # drop avgpool & fc
        
        if global_pool.lower() == 'max':
            self.global_pool_layer = nn.AdaptiveMaxPool2d((1, 1))
            print("ðŸ“Š Using Global Max Pooling")
        else:
            self.global_pool_layer = nn.AdaptiveAvgPool2d((1, 1))
            print("ðŸ“Š Using Global Average Pooling")
        
        if dropout_rate > 0:
            self.classifier = nn.Sequential(
                nn.Dropout(dropout_rate),
                nn.Linear(num_features, num_classes)
            )
            print(f"ðŸ“Š Added dropout layer: {dropout_rate}")
        else:
            self.classifier = nn.Linear(num_features, num_classes)
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        """Apply Kaiming initialization."""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                nn.init.constant_(m.bias, 0)
        print("âœ… Applied Kaiming initialization (paper standard)")
    
    def forward(self, x):
        x = self.resnet(x)
        x = self.global_pool_layer(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x

def train_model():
    """
    Main training function - PAPER COMPLIANT VERSION.

    Paper settings:
    - SGD + momentum 0.9
    - Weight decay 1e-4
    - LR starts at 0.1, /10 on plateau
    - Train augment: Scale jitter (shorter side in [256,480]) + 224 crop + flip
    """
    hyperparams = get_hyperparameters()
    
    current_iteration = int(os.environ.get('ITERATION', 0))
    step = int(os.environ.get('STEP', 8))
    
    if current_iteration < step:
        phase = 1
        optimizing = "learning_rate"
    elif current_iteration < step * 2:
        phase = 2
        optimizing = "batch_size/weight_decay"
    else:
        phase = 3
        optimizing = "dropout_rate/global_pool"
    
    print(f"ðŸš€ Iteration {current_iteration} | Phase {phase} | Optimizing: {optimizing}")
    
    learning_rate = hyperparams.get('learning_rate', 0.1)
    batch_size = hyperparams.get('batch_size', 32)
    epochs = hyperparams.get('epochs', 50)
    weight_decay = hyperparams.get('weight_decay', 0.0001)
    momentum = hyperparams.get('momentum', 0.9)
    optimizer_type = hyperparams.get('optimizer', 'SGD')
    dropout_rate = hyperparams.get('dropout_rate', 0.0)
    global_pool = hyperparams.get('global_pool', 'avg')
    data_dir = hyperparams.get('data_dir', './butterfly_data')
    scale_jitter_min = hyperparams.get('scale_jitter_min', 256)
    scale_jitter_max = hyperparams.get('scale_jitter_max', 480)
    
    print(f"ðŸ“Š Paper-compliant settings:")
    print(f"   LR: {learning_rate}, BS: {batch_size}, WD: {weight_decay}, Momentum: {momentum}")
    print(f"   Optimizer: {optimizer_type}, Global Pool: {global_pool}, Dropout: {dropout_rate}")
    print(f"   Scale Jitter Range (shorter side): [{scale_jitter_min}, {scale_jitter_max}]")
    
    if optimizer_type.lower() != 'sgd':
        print(f"âš ï¸ WARNING: Using {optimizer_type}, but paper specifies SGD")
    if momentum != 0.9:
        print(f"âš ï¸ WARNING: Using momentum {momentum}, but paper specifies 0.9")
    
    train_loader, val_loader, test_loader, num_classes, class_names = load_butterfly_data(
        data_dir, batch_size, scale_jitter_min=scale_jitter_min, scale_jitter_max=scale_jitter_max
    )
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ðŸ–¥ï¸ Device: {device} | Classes: {num_classes}")
    
    model = ResNetPaperCompliant(
        num_classes=num_classes, 
        dropout_rate=dropout_rate,
        global_pool=global_pool,
        pretrained=True
    ).to(device)
    
    criterion = nn.CrossEntropyLoss()
    
    if optimizer_type.lower() == 'sgd':
        optimizer = optim.SGD(
            model.parameters(), 
            lr=learning_rate, 
            momentum=momentum, 
            weight_decay=weight_decay
        )
        print(f"âœ… Using paper-compliant SGD optimizer")
    else:
        print(f"âš ï¸ Using {optimizer_type} (not paper-compliant)")
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.1, patience=10
    )
    
    print(f"ðŸš€ Starting paper-compliant training for {epochs} epochs")
    print(f"ðŸ“š Following: He et al. 'Deep Residual Learning for Image Recognition' (CVPR 2016)")
    
    train_losses, val_losses = [], []
    train_accuracies, val_accuracies = [], []
    epoch_list = []
    
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (inputs, labels) in enumerate(train_loader):
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        
        train_loss = running_loss / len(train_loader)
        train_accuracy = correct / total
        val_loss, val_accuracy = evaluate_model(model, val_loader, criterion, device)
        scheduler.step(val_loss)
        
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accuracies.append(train_accuracy)
        val_accuracies.append(val_accuracy)
        epoch_list.append(epoch + 1)
        
        if (epoch + 1) % 10 == 0 or epoch == epochs - 1:
            current_lr = optimizer.param_groups[0]['lr']
            print(f"Epoch {epoch+1}/{epochs} - Train: {train_accuracy:.3f}, Val: {val_accuracy:.3f}, LR: {current_lr:.6f}")
    
    has_test_labels = True
    try:
        sample_batch = next(iter(test_loader))
        sample_labels = sample_batch[1]
        if all(label == 0 for label in sample_labels) and len(set(sample_labels.numpy())) == 1:
            has_test_labels = False
    except:
        has_test_labels = True
    
    test_loss, test_accuracy = evaluate_model_safe(model, test_loader, criterion, device, has_test_labels)
    
    print(f"\nðŸŽ¯ Final Results (Paper-Compliant ResNet-18):")
    print(f"   Train Acc: {train_accuracies[-1]:.4f} | Val Acc: {val_accuracies[-1]:.4f}")
    if has_test_labels:
        print(f"   Test Acc: {test_accuracy:.4f}")
    
    print(f"\nðŸ“š Paper Compliance Summary:")
    print(f"   âœ… SGD optimizer with momentum {momentum}")
    print(f"   âœ… Weight decay: {weight_decay}")
    print(f"   âœ… Scale jittering (shorter side âˆˆ [{scale_jitter_min},{scale_jitter_max}]) + RandomCrop(224) + HorizontalFlip")
    print(f"   âœ… Kaiming initialization")
    print(f"   âœ… Configurable global pooling: {global_pool}")
    print(f"   âœ… ReduceLROnPlateau (paper-style LR schedule)")
    
    optimized_used = []
    if os.environ.get('OPTIMIZED_LEARNING_RATE'):
        optimized_used.append(f"LR={os.environ.get('OPTIMIZED_LEARNING_RATE')}")
    if os.environ.get('OPTIMIZED_BATCH_SIZE'):
        optimized_used.append(f"BS={os.environ.get('OPTIMIZED_BATCH_SIZE')}")
    if os.environ.get('OPTIMIZED_WEIGHT_DECAY'):
        optimized_used.append(f"WD={os.environ.get('OPTIMIZED_WEIGHT_DECAY')}")
    if os.environ.get('OPTIMIZED_DROPOUT_RATE'):
        optimized_used.append(f"DR={os.environ.get('OPTIMIZED_DROPOUT_RATE')}")
    if os.environ.get('OPTIMIZED_GLOBAL_POOL'):
        optimized_used.append(f"GP={os.environ.get('OPTIMIZED_GLOBAL_POOL')}")
    if optimized_used:
        print(f"ðŸŽ¯ Optimized parameters: {', '.join(optimized_used)}")
    
    metrics = {
        'train_loss': train_losses,
        'val_loss': val_losses,
        'train_accuracy': train_accuracies,
        'val_accuracy': val_accuracies,
        'test_loss': test_loss,
        'test_accuracy': test_accuracy,
        'paper_compliant': True,
        'global_pool': global_pool,
        'final_learning_rate': optimizer.param_groups[0]['lr'],
        'scale_jitter_min': scale_jitter_min,
        'scale_jitter_max': scale_jitter_max
    }
    
    save_results(metrics, epoch_list)
    print("âœ… Paper-compliant training complete.")
    
    return model, val_accuracies[-1]

if __name__ == "__main__":
    try:
        print("ðŸš€ Starting ResNet-18 Training (Paper-Compliant Version)")
        print("ðŸ“š Based on: He et al. 'Deep Residual Learning for Image Recognition' (CVPR 2016)")
        print("ðŸ”§ Key notes:")
        print("   - Added Scale Jittering (shorter side âˆˆ [256, 480]) per paper")
        print("   - Strict paper-compliant augmentation + SGD settings")
        print("   - Global pooling tunable; Kaiming init")
        print()
        
        train_model()
    except Exception as e:
        print(f"âŒ Training failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
